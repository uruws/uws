* Last 7 days [changelog][weekly-changelog]

[weekly-changelog]: https://github.com/TalkingPts/Infrastructure/compare/master%40%7B7days%7D...master

---

* Infra Development Backlog - `WAITING`
    * App Cache-Control header for CDN assets - [DEV-9810][DEV-9810]
    * App api clients domain - [DEV-9833][DEV-9833]

[DEV-9810]: https://talkingpointsorg.atlassian.net/browse/DEV-9810
[DEV-9833]: https://talkingpointsorg.atlassian.net/browse/DEV-9833

---

* rotate aws ses credentials due to crond env vars exposing them - `DONE!` [PR237][PR237]
    * only internally, but still
    * secret/aws.iam/smtps_credentials.csv

[PR237]: https://github.com/TalkingPts/Infrastructure/pull/237

---

* eks: encrypt cluster secrets - `DONE!` [PR238][PR238]
    * encrypt data on storage too

[PR238]: https://github.com/TalkingPts/Infrastructure/pull/238

---

* cluster: new pnt-2308 - `DONE!` [PR239][PR239]
    * new cluster for pentests

[PR239]: https://github.com/TalkingPts/Infrastructure/pull/239

---

* tapo: sarmiento pentest env - `DONE!` [PR240][PR240]
    * pnt-2308 cluster

[PR240]: https://github.com/TalkingPts/Infrastructure/pull/240

---

* tapo: sarmiento pentest setup
    * change to tapo.uno domain?
        * we need new TLS certs for that
    * currently it's using staging settings, it should be separate

---

* msmtprc: profiled setup
    * generate files in order to identify emails origin
    * currently all mails are from user@uws.t.o
    * we could use something like user-cluster@uws.t.o or user-host@uws.t.o or similar

---

* munin: deprecate apijob (worker jobs)
    * it will be done from infra-ui project

---

* meteor secrets: pull from heroku
    * pull heroku env vars for aws deploy/restart
    * remove: secret/eks/files/meteor

---

* api munin
    * check https://api.talkingpts.org/ (404)
    * check https://api.talkingpts.org/api/
    * check https://api.talkingpts.org/bandwidthCallbackSMS
    * check https://api.talkingpts.org/coconut_webhook

---

* k8s clusters
    * migrate appsprod-2302 services (CS, infra-ui, ...)
        * DEGRADED nodegroup
        * move services to appwrk-2306
        * remove appsprod-2302 cluster after migrating all services

---

* docker: mailx via amazon-ses
    * setup msmtp to send emails via amazon-ses
    * msmtprc should be templated/generated by environment
        * like jsbatch
        * container X
        * k8s services
        * etc

---

* CA: deprecate smtps

---

* Buildpack:
    * add start/end date time info to build logs to track elapsed time
    * currently two log files are created, should be only one
        * one for make _app_
        * other for the make deploy
        * if app-build sets the LOGFILE env var makefiles could use that one

---

* k8s: prom+grafana cluster local dashboard for on-the-fly metrics

---

* haproxy: metrics
    * integrate with k8s dashboard
    * integrate with munin

---

* k8smon: munin network usage graphs
    * by namespace
    * by node
    * total
    * by pod/container?

---

* 2309 [upgrades][upgrades] round

[upgrades]: ./infra/upgrades.md

---

* uwscli: meteor deploy to heroku to keep versions in sync

---

* uwscli new build/console server
    * run autobuilds there
    * with more CPU and MEM
    * do we move uwscli tools to there?
    * or do we make app-build dispatch the build on the build server?

---

* uwscli: nq deploy so once they start can not be stopped by ctrl+c or similar
    * we mainly need it for when custom deploys are launched
    * so maybe we have to nq deploys and also the custom deploy too

---

* `SEC` `CRITICAL` infra internal domain
	* get a new and separate domain to isolate from main domain
	* we should replace uws.t.o with this new domain
	* we should also setup mailx for this domain to also separate from main domain accounts
		* we should use this domain for critical/internal/3rd party accounts
		* AWS accounts in example should use this separate domain
		* any 3rd party integration (Jira, Bandwidth, etc, etc, etc) should use accounts on this domain
		* all these mainly due to the Ramp integration the Finance people asked about which requires read access to ALL domain accounts

---

* meteor: accelerate build time using local caches/proxy/repo for npm dependencies

---

* munin: check that App response headers include the "security headers" we need for SOC2
    * https://staging.t.o/login
        * Content-Security-Policy
        * Cross-Origin-Resource-Policy
        * Access-Control-Allow-Origin
        * Referrer-Policy

---

* munin: alert about workers callback http errors [worker-errors][worker-errors]
    * warning at 3 errors per minute
    * critical at 5 errors per minute
    * send alert to status page

[worker-errors]: https://worker-2209.uws.talkingpts.org/munin/uws/worker-2209/web_request_worker_uws_talkingpts_org/errors_per_minute.html

---

* `SEC` App security changes
    * Remove private/settings.json from the repo

---

* `SEC` `FIX` Firebase content is discoverable:
    * https://firebasestorage.googleapis.com/v0/b/talkingpnts.appspot.com/o/
    * working in an unrelated issue with Gabriel we found out that ^
    * we should use a CDN for that too, and avoid this issue and also improve performance and costs

---

* munin cluster's storage archive/sync to jsbatch or similar
    * for archiving/history and backup purposes
    * also to have some data available in case of the cluster being down
        * even if it will be old data, could be useful still
    * graph for nodes region/zones
    * nodes types
        * pass INSTANCE_TYPES env var to munin-node and keep them in the graph
    * graphs for PVs/PVCs?
        * available storage
        * region/zones

---

* app-autoscale
    * implement auto scaler based on custom metrics
        * web: scale based on web traffic/requests per minute
            * more or less, nowadays, 10 containers to serve 1000 rpm
            * [DB query](./worker-check-scheduled-jobs.txt)
        * workers:
            * cleverSynch could/should be another metric as it needs to finish before 10hs UTC
            * messages.jobs ready?
            * one metric could be based on checking scheduled district jobs to scale up before it starts, instead of reacting to an alert

---

* forensics setup

---

* CA rotate ops/210823

---

* Research Team
    * re-implement jupyter notebook setups
        * setup one web interface per user vs the "global" one we currently have

---

* munin
    * graph/check nodegroup status (alert when it's DEGRADED or not ACTIVE)
    * uwseks get nodegroup -n main -o json
    * check that munin-node container/service is running via k8smon "proxy"

---

* implement [node inspector][node-inspector] setup using the SSH approach
    * the idea is to have a way to dispatch "debugging containers" which could be inspected using chrome dev tools or similar, using the SSH tunneling setup as it seems to be the more secure for our infra
    * the SSH service should be only enabled when we dispatch the debug containers, using a random public port ideally and setting a random password for the uws user inside the container (show that password in the container init steps or similar) so we can share that info (port and password) "securely" as it's generated every time the container starts

[node-inspector]: https://nodejs.org/en/docs/guides/debugging-getting-started/

---

* `FIX` implement a "double check" mechanism for changing DNS uws.t.o domain records
    * the idea is to avoid issues like the one I did changing a production record
    * maybe use an script for Route53 editions which alerts about prod domains or similar
    * try to avoid manual changes (maybe some peer review?)

---

* munin: scan cluster ingress domains and add them to the checks (munin-node-clusters)

---

* aws support meeting
    * Route53 app.t.o use geolocation inside US or latency setup
        * versus current weighted 50/50 setup
        * we must keep the "heroku contingency plan" setup or adapt it to new ways

---

* `FIX` buildpack:
    * use tag version from command line for publishing the image
    * instead of using the git describe tag
    * currently if a commit has more than one tag associated build fails because previous version already exists
    * that or fix the git describe command to get latest tag instead of first one

    tag invalid: The image tag 'meteor-app-2.64.7-bp21' already exists in the
    'uws' repository and cannot be overwritten because the repository is immutable.

    Publish app version 2.64.8 failed

---

* uwscli:
    * cleanup old images in ECR
    * app-build
        * we should be able to properly stop/abort a building process
    * show events log or auto-refresh status info
    * control deploy replicas
    * app-build
        * keep a "queued list"
        * only build tag not in "already done list" nor in the "queued list" either
        * to avoid all the duplicate build jobs
    * cli/buildpack.sh: should manage the log and email report if any fail
    * cli/app-build.sh: should do the same

---

* uwsq: clean failed jobs

---

* `SEC` mongodb credentials rotation schedule

---

* `SEC` aws auth credentials rotation schedule
    * uwsadm and friends "access keys"

---

* munin pods_container (check phase)
    "status": {
        "message": "The node was low on resource: memory. Container controller was using 2391212Ki, which exceeds its request of 90Mi. ",
        "phase": "Failed",
        "reason": "Evicted",
        "startTime": "2022-02-09T03:24:23Z"
    }

---

* munin alerts to slack
    * setup/devel bot
    * remove setup munin limit mail alerts
        * dev_ops_vo548nvb
            * munin-alerts TO
            * gmail fetch
            * create forward rules to slack and others

---

* rstudio checks
    * http_loadtime IDE and Jupyter Notebook from jsbatch
    * vm local munin setup (ansible role)

---

* ansible roles
    * monit
        * setup monit to check fail2ban keeps running
        * and others...
    * fail2ban
    * munin

---

* non-prod sites robots.txt to disallow all crawlers?

---

* WAF setup
    * implement fail2ban for kubernets/aws?
    * nginx modsecurity
        * https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/configmap.md#enable-modsecurity
        * https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md#modsecurity

---

* infra docs for internal presentation

---

* block web access by geoip?
    * https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/configmap.md#use-geoip
